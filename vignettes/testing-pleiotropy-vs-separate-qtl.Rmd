---
title: "Testing pleiotropy v separate QTL in multiparental populations"
author: "Frederick Boehm"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Testing pleiotropy v separate QTL in multiparental populations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  tidy = TRUE, 
  tidy.opts = list(width.cutoff = 60)
  )

```

## Setting for pleiotropy vs. two separate QTL 

Our setting involves a pair of traits, $Y_1$ and $Y_2$, each of which individually (univariately) maps to a single genomic region. $Y_1$ and $Y_2$ are both measured on the same subjects. The exact definition of a genomic region is imprecise; in practice, it may be as large as 4 or 5 Mb. We seek to distinguish whether $Y_1$ and $Y_2$ associations (in the genomic region of interest) arise due to a single QTL or whether there are two two distinct loci, each of which associates with exactly one of the two traits. We recognize that more complicated association patterns are possible, but we neglect them in this test.

## Installing `qtl2pleio`

We install `qtl2pleio` from github via the `devtools` R package, which is available from CRAN. 

If you haven't installed `devtools` R package, you can do so with this line of code:

```{r, eval = FALSE}
install.packages("devtools")
```

Now that you have installed `devtools`, you can install `qtl2pleio` from its Github repository with this line of code:


```{r, eval = FALSE}
devtools::install_github("fboehm/qtl2pleio")
```

The above line only needs to be run once on a given computer (unless you wish to install a newer version of the package). 

We then load the library into our R session with the `library` command:

```{r}
library(qtl2pleio)
```


We'll work with data from the `qtl2data` repository, which is on github. First, we install and load the `qtl2` package.

```{r, eval = FALSE}
devtools::install_github("rqtl/qtl2")
```

We use the above line once to install the package on our computer before loading the package with the `library` command.

```{r}
library(qtl2)
```

## Reading data from `qtl2data` repository on github

We read from github.com data from the `qtl2data` repository.

```{r}
tmpfile <- tempfile()
file <- paste0("https://raw.githubusercontent.com/rqtl/",
               "qtl2data/master/DOex/DOex_alleleprobs.rds")
download.file(file, tmpfile)
pr <- readRDS(tmpfile)
unlink(tmpfile)
tmpfile <- tempfile()
file <- paste0("https://raw.githubusercontent.com/rqtl/",
               "qtl2data/master/DOex/DOex_pmap.csv")
download.file(file, tmpfile)
pmap_pre <- read.csv(tmpfile)
unlink(tmpfile)
pm2 <- pmap_pre[pmap_pre$chr == 2, 3]
names(pm2) <- pmap_pre[pmap_pre$chr == 2, 1]
pm3 <- pmap_pre[pmap_pre$chr == 3, 3]
names(pm3) <- pmap_pre[pmap_pre$chr == 3, 1]
pmX <- pmap_pre[pmap_pre$chr == "X", 3]
names(pmX) <- pmap_pre[pmap_pre$chr == "X", 1]
list(pm2, pm3, pmX) -> pm
names(pm) <- c("`2`", "`3`", "X")
```

We now have an allele probabilities object stored in `pr`.

```{r}
names(pr)
dim(pr$`2`)
```

We see that `pr` is a list of 3 three-dimensional arrays - one array for each of 3 chromosomes.

## Kinship calculations

For our statistical model, we need a kinship matrix. Although we don't have genome-wide data - since we have allele probabilities for only 3 chromosomes - let's calculate a kinship matrix using "leave-one-chromosome-out". In practice, one would want to use allele probabilities from a full genome-wide set of markers.

```{r}
calc_kinship(probs = pr, type = "loco") -> kinship
```

```{r}
str(kinship)
```

We see that `kinship` is a list containing 3 matrices. Each matrix is 261 by 261 - where the number of subjects is 261 - and symmetric. The $(i, j)$ cell in the matrix contains the estimate of identity-by-state (IBS) probability for randomly chosen alleles at a single locus for those two subjects.




Before we simulate phenotype data, we first specify our statistical model.

We use the model:

$$vec(Y) = X vec(B) + vec(G) + vec(E)$$

where $Y$ is a $n$ by $2$ matrix, where each row is one subject and each column is one quantitative trait. $X$ is a $2n$ by $2f$ design matrix containing $n$ by $f$ allele probabilities matrices for each of two (possibly identical) markers. Thus, $X$ is a block-diagonal matrix, with exactly two $n$ by $f$ blocks on the diagonal. $B$ is a $f$ by 2 matrix. "vec" refers to the vectorization operator. "vec(B)", where $B$ is a $f$ by $2$ matrix, is, thus, a (column) vector of length $2f$ that is formed by stacking the second column of $B$ beneath the first column of $B$. 

$G$ is a matrix of random effects. We specify its distribution as matrix-variate normal with mean being a $n$ by $2$ matrix of zeros, covariance among row vectors a $n$ by $n$ kinship matrix, $K$, and covariance among column vectors a $2$ by $2$ genetic covariance matrix, $V_g$. 

In mathematical notation, we write:

$$G \sim MN_{\text{n by 2}}(0, K, V_g)$$

We also need to specify the distribution of the $E$ matrix, which contains the random errors. $E$ is a random $n$ by $2$ matrix that is distributed as a matrix-variate normal distribution with mean being the $n$ by $2$ zero matrix, covariance among row vectors $I_n$, the $n$ by $n$ identity matrix, and covariance among columns the $2$ by $2$ matrix $V_e$.

$$E \sim MN_{\text{n by 2}}(0, I_n, V_e)$$



In practice, we typically observe the phenotype matrix $Y$. We also treat as known the design matrix $X$ and the kinship matrix $K$. We then infer the values of $B$, $V_g$, and $V_e$. 



## Simulating phenotypes with `qtl2pleio::sim1`


The function to simulate phenotypes in `qtl2pleio` is `sim1`. By examining its help page, we see that it takes five arguments. The help page also gives the dimensions of the inputs.

```{r}
# set up the design matrix, X
pp <- pr[[2]]
X <- gemma2::stagger_mats(pp[ , , 50], pp[ , , 50])
# assemble B matrix of allele effects
B <- matrix(data = c(-1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1), nrow = 8, ncol = 2, byrow = FALSE)
# verify that B is what we want:
B
# set.seed to ensure reproducibility
set.seed(2018-01-30)
# call to sim1
Ypre <- sim1(X = X, B = B, Vg = diag(2), Ve = diag(2), kinship = kinship[[2]])
Y <- matrix(Ypre, nrow = 261, ncol = 2, byrow = FALSE)
rownames(Y) <- rownames(pp)
colnames(Y) <- c("tr1", "tr2")
```

Let's perform univariate QTL mapping for each of the two traits in the Y matrix.

```{r}
scan1(genoprobs = pr, pheno = Y, kinship = kinship) -> s1
```

```{r}
plot(s1, pm)
```


```{r}
find_peaks(s1, map = pm)
```





# Perform two-dimensional scan as first step in pleiotropy v separate QTL hypothesis test


```{r}
out <- scan_pvl(probs = pp, 
                pheno = Y, 
                kinship = kinship[[2]], # 2nd entry in kinship list is Chr 3
                start_snp = 38, 
                n_snp = 25, n_cores = 1
                )
```


### Create a profile LOD plot to visualize results of two-dimensional scan

```{r}
library(dplyr)
out %>% 
  tidy_scan_pvl(pm3) %>% # pm3 is physical map for Chr 3
  add_intercepts(intercepts_univariate = c(82.8, 82.8)) %>%
  plot_pvl(phenames = c("tr1", "tr2"))

```


### Calculate the likelihood ratio test statistic for pleiotropy v separate QTL

We use the function `calc_lrt_tib` to calculate the likelihood ratio test statistic value for the specified traits and specified genomic region.

```{r}
(calc_lrt_tib(out) -> lrt)
```

## Bootstrap analysis to get p-values

The calibration of test statistic values to get p-values uses bootstrap methods because we don't know the theoretical distribution of the test statistic under the null hypothesis. Thus, we use a bootstrap approach to obtain an empirical distribution of test statistic values under the null hypothesis of the presence of one pleiotropic locus. 

We will use the function `boot_pvl` from our package `qtl2pleio`.

We use a parametric bootstrap strategy in which we first use the studied phenotypes to infer the values of model parameters. Once we have the inferred values of the model parameters, we simulate phenotypes from the pleiotropy model (with the inferred parameter values). 


A natural question that arises is "which marker's allele probabilities do we use when simulating phenotypes?" We use the marker that, under the null hypothesis, ie, under the pleiotropy constraint, yields the greatest value of the log-likelihood.

Before we call `boot_pvl`, we need to identify the index (on the chromosome under study) of the marker that maximizes the likelihood under the pleiotropy constraint. To do this, we use the `qtl2pleio` function `find_pleio_peak_tib`. 

```{r}
(pleio_index <- find_pleio_peak_tib(out, start_snp = 38))
```




```{r}
set.seed(2018-10-03)
system.time(b_out <- boot_pvl(probs = pp,
         pheno = Y, 
         pleio_peak_index = pleio_index, 
         kinship = kinship[[2]], # 2nd element in kinship list is Chr 3 
         nboot_per_job = 10, 
         start_snp = 38, 
         n_snp = 10
         ))

```


The argument `nboot_per_job` indicates the number of bootstrap samples that will be created and analyzed. Here, we set `nboot_per_job = 10`, so we expect to see returned a numeric vector of length 10, where each entry is a LRT statistic value from a distinct bootstrap sample.

Finally, we determine a bootstrap p-value in the usual method. We treat the bootstrap samples' test statistics as an empirical distribution of the test statistic under the null hypothesis of pleiotropy. Thus, to get a p-value, we want to ask "What is the probability, under the null hypothesis, of observing a test statistic value that is at least as extreme as that which we observed?"

```{r}
(pvalue <- mean(b_out >= lrt))
```

Typically, one would want to conduct the two-dimensional scan over a larger grid (ie, with more markers). We abbreviated the scan above to reduce computing time.

In practice, one would want to use many more bootstrap samples to achieve an empirical distribution that is closer to the theoretical distribution of the test statistic under the null hypothesis. 

However, if one wants to perform analyses with a reasonable number - say 400 - bootstrap samples, this will take a very long time - several days - on a single laptop computer. We have used a series of computer clusters that are coordinated by the University of Wisconsin-Madison's Center for High-throughput Computing (http://chtc.cs.wisc.edu). We typically are able to analyze 1000 bootstrap samples in less than 24 hours with this service.

We recently added parallel computing capacity with `boot_pvl` and `scan_pvl`. The user may specify `n_cores` when calling each function. 





## Session info

```{r}
devtools::session_info()
```


